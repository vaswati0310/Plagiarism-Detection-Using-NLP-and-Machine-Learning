{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Word2Vec model loaded successfully!\n",
      "Loading RoBERTa model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa model loaded successfully!\n",
      "Loading BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eca50361069468790eb8f52bb932c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  52%|#####2    | 231M/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Roy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Roy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\_monitor.py\", line 84, in run\n",
      "    instance.refresh(nolock=True)\n",
      "  File \"c:\\Users\\Roy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py\", line 1347, in refresh\n",
      "    self.display()\n",
      "  File \"c:\\Users\\Roy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\notebook.py\", line 171, in display\n",
      "    rtext.value = right\n",
      "    ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Roy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\traitlets\\traitlets.py\", line 716, in __set__\n",
      "    self.set(obj, value)\n",
      "  File \"c:\\Users\\Roy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\traitlets\\traitlets.py\", line 706, in set\n",
      "    obj._notify_trait(self.name, old_value, new_value)\n",
      "  File \"c:\\Users\\Roy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\traitlets\\traitlets.py\", line 1513, in _notify_trait\n",
      "    self.notify_change(\n",
      "  File \"c:\\Users\\Roy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipywidgets\\widgets\\widget.py\", line 700, in notify_change\n",
      "    self.send_state(key=name)\n",
      "  File \"c:\\Users\\Roy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipywidgets\\widgets\\widget.py\", line 586, in send_state\n",
      "    self._send(msg, buffers=buffers)\n",
      "  File \"c:\\Users\\Roy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipywidgets\\widgets\\widget.py\", line 825, in _send\n",
      "    self.comm.send(data=msg, buffers=buffers)\n",
      "  File \"c:\\Users\\Roy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\comm\\base_comm.py\", line 144, in send\n",
      "    self.publish_msg(\n",
      "  File \"c:\\Users\\Roy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\comm\\comm.py\", line 42, in publish_msg\n",
      "    parent=self.kernel.get_parent(),\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Roy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 797, in get_parent\n",
      "    return self._shell_parent.get()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "LookupError: <ContextVar name='shell_parent' at 0x000001B7CDAE7510>\n",
      "c:\\Users\\Roy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Roy\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model loaded successfully!\n",
      "\n",
      "Plagiarism Detection Tool (Word2Vec + BERT + RoBERTa Hybrid)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9efaaa433f4c6596a82050dd6afef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Input:', layout=Layout(width='500px'), placeholder='Enter a sentence to check for â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1060fa90df445b8b216ed4f79589698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Classify', style=ButtonStyle(), tooltip='Click to classify the senâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671ace57d067458ea7988ed0f993e81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from transformers import RobertaTokenizer, RobertaModel, BertTokenizer, BertModel  # ðŸ†• Added Bert\n",
    "import torch\n",
    "import re\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Repository sentences (reference database)\n",
    "repository_sentences = [\n",
    "    \"artificial intelligence is transforming industries by automating tasks and improving efficiency\",\n",
    "    \"climate change is a global challenge that requires immediate attention to mitigate its adverse effects\",\n",
    "    \"the rapid advancement of technology has significantly impacted communication and information sharing\",\n",
    "    \"renewable energy sources such as solar and wind are crucial for reducing dependence on fossil fuels\",\n",
    "    \"data privacy and security have become major concerns in the digital age requiring stringent measures\"\n",
    "]\n",
    "repo_tokens = [sent.split() for sent in repository_sentences]\n",
    "\n",
    "# --- Load Word2Vec (with retry mechanism) ---\n",
    "max_attempts = 3\n",
    "for attempt in range(max_attempts):\n",
    "    try:\n",
    "        word2vec_model = api.load('word2vec-google-news-300')\n",
    "        print(\"Word2Vec model loaded successfully!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "        if attempt < max_attempts - 1:\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# --- Sentence vector using Word2Vec ---\n",
    "def get_sentence_vector(tokens, model):\n",
    "    vectors = [model[word] for word in tokens if word in model]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# --- Load RoBERTa model ---\n",
    "print(\"Loading RoBERTa model...\")\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "roberta_model.eval()\n",
    "print(\"RoBERTa model loaded successfully!\")\n",
    "\n",
    "# --- Load BERT model ---  ðŸ†•\n",
    "print(\"Loading BERT model...\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "bert_model.eval()\n",
    "print(\"BERT model loaded successfully!\")\n",
    "\n",
    "# --- Get embedding using RoBERTa [CLS] (<s>) token ---\n",
    "def get_roberta_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "\n",
    "# --- Get embedding using BERT [CLS] token ---  ðŸ†•\n",
    "def get_bert_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "\n",
    "# --- Plagiarism Classification Function ---\n",
    "def classify_plagiarism(input_sentence, repo_sentences, repo_tokens,\n",
    "                        word2vec_model, roberta_model, roberta_tokenizer,\n",
    "                        bert_model, bert_tokenizer):  # ðŸ†• added BERT params\n",
    "    input_sentence = re.sub(r'[^\\w\\s]', '', input_sentence.lower())\n",
    "    input_tokens = input_sentence.split()\n",
    "\n",
    "    features = []\n",
    "    valid_indices = []\n",
    "    for i, (repo_sent, repo_tok) in enumerate(zip(repo_sentences, repo_tokens)):\n",
    "        # Word2Vec similarity\n",
    "        input_vec = get_sentence_vector(input_tokens, word2vec_model)\n",
    "        repo_vec = get_sentence_vector(repo_tok, word2vec_model)\n",
    "        w2v_sim = np.dot(input_vec, repo_vec) / (np.linalg.norm(input_vec) * np.linalg.norm(repo_vec) + 1e-8)\n",
    "\n",
    "        # RoBERTa similarity\n",
    "        input_roberta = get_roberta_embedding(input_sentence, roberta_tokenizer, roberta_model)\n",
    "        repo_roberta = get_roberta_embedding(repo_sent, roberta_tokenizer, roberta_model)\n",
    "        roberta_sim = np.dot(input_roberta, repo_roberta) / (np.linalg.norm(input_roberta) * np.linalg.norm(repo_roberta) + 1e-8)\n",
    "\n",
    "        # BERT similarity  ðŸ†•\n",
    "        input_bert = get_bert_embedding(input_sentence, bert_tokenizer, bert_model)\n",
    "        repo_bert = get_bert_embedding(repo_sent, bert_tokenizer, bert_model)\n",
    "        bert_sim = np.dot(input_bert, repo_bert) / (np.linalg.norm(input_bert) * np.linalg.norm(repo_bert) + 1e-8)\n",
    "\n",
    "        # Word overlap & length difference\n",
    "        word_overlap = len(set(input_tokens) & set(repo_tok)) / len(set(input_tokens))\n",
    "        length_diff = abs(len(input_tokens) - len(repo_tok))\n",
    "\n",
    "        # Skip very low similarity cases early\n",
    "        if (w2v_sim < 0.6 and bert_sim < 0.6 and roberta_sim < 0.6) or word_overlap < 0.2:\n",
    "            continue\n",
    "\n",
    "        features.append([w2v_sim, bert_sim, roberta_sim, word_overlap, length_diff])\n",
    "        valid_indices.append(i)\n",
    "\n",
    "    if not features:\n",
    "        return \"No Match\", None, None, 0.0, 0.0, 0.0\n",
    "\n",
    "    features = np.array(features)\n",
    "    # Combine all three similarities (weights can be tuned) ðŸ†•\n",
    "    combined_sim = features[:, 0]*0.3 + features[:, 1]*0.3 + features[:, 2]*0.4\n",
    "    best_match_idx = np.argmax(combined_sim)\n",
    "    w2v_sim, bert_sim, roberta_sim, word_overlap, length_diff = features[best_match_idx]\n",
    "\n",
    "    # --- Classification Rules ---\n",
    "    if all(s > 0.98 for s in [w2v_sim, bert_sim, roberta_sim]) and word_overlap > 0.95 and length_diff == 0:\n",
    "        predicted_class = \"Cut-Paste\"\n",
    "    elif min(w2v_sim, bert_sim, roberta_sim) >= 0.85 and word_overlap >= 0.6:\n",
    "        predicted_class = \"Light Paraphrasing\"\n",
    "    elif min(w2v_sim, bert_sim, roberta_sim) >= 0.6 and word_overlap < 0.6:\n",
    "        predicted_class = \"Heavy Paraphrasing\"\n",
    "    else:\n",
    "        predicted_class = \"No Match\"\n",
    "\n",
    "    similarity_percentage = (w2v_sim*0.3 + bert_sim*0.3 + roberta_sim*0.3 + word_overlap*0.1) * 100\n",
    "    matched_sentence = repo_sentences[valid_indices[best_match_idx]]\n",
    "\n",
    "    return predicted_class, matched_sentence, features[best_match_idx], similarity_percentage, bert_sim, roberta_sim  # ðŸ†• return both\n",
    "\n",
    "# --- Widgets UI for interactive testing ---\n",
    "input_text = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter a sentence to check for plagiarism',\n",
    "    description='Input:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "classify_button = widgets.Button(\n",
    "    description='Classify',\n",
    "    button_style='success',\n",
    "    tooltip='Click to classify the sentence',\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_classify_button_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        input_sentence = input_text.value.strip()\n",
    "        if not input_sentence:\n",
    "            print(\"Please enter a sentence.\")\n",
    "            return\n",
    "        \n",
    "        predicted_class, matched_sentence, features, similarity_percentage, bert_sim, roberta_sim = classify_plagiarism(\n",
    "            input_sentence, repository_sentences, repo_tokens,\n",
    "            word2vec_model, roberta_model, roberta_tokenizer,\n",
    "            bert_model, bert_tokenizer\n",
    "        )\n",
    "        \n",
    "        print(f\"Input Sentence: {input_sentence}\")\n",
    "        print(f\"Matched Sentence: {matched_sentence if matched_sentence else 'None'}\")\n",
    "        print(f\"Predicted Plagiarism Type: {predicted_class}\")\n",
    "        print(f\"Overall Similarity: {similarity_percentage:.2f}%\")\n",
    "        \n",
    "        if features is not None:\n",
    "            w2v_sim, bert_sim, roberta_sim, word_overlap, length_diff = features\n",
    "            print(f\"\\n--- Detailed Similarity Metrics ---\")\n",
    "            print(f\"â€¢ Word2Vec Similarity:  {w2v_sim:.4f}\")\n",
    "            print(f\"â€¢ BERT Similarity:      {bert_sim:.4f}\")      # ðŸ†• Added\n",
    "            print(f\"â€¢ RoBERTa Similarity:   {roberta_sim:.4f}\")\n",
    "            print(f\"â€¢ Word Overlap:          {word_overlap:.4f}\")\n",
    "            print(f\"â€¢ Length Difference:     {length_diff}\")\n",
    "        else:\n",
    "            print(\"No features available (no valid match found).\")\n",
    "\n",
    "classify_button.on_click(on_classify_button_clicked)\n",
    "\n",
    "print(\"\\nPlagiarism Detection Tool (Word2Vec + BERT + RoBERTa Hybrid)\\n\")\n",
    "display(input_text, classify_button, output_area)\n",
    "\n",
    "# Test example\n",
    "input_text.value = \"artificial intelligence is modifying industries by automating jobs and enhancing performance\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'elasticsearch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipywidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwidgets\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, clear_output\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01melasticsearch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Elasticsearch\n\u001b[32m     12\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'elasticsearch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import re\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "if not es.ping():\n",
    "    raise ValueError(\"Elasticsearch connection failed!\")\n",
    "print(\"Connected to Elasticsearch\")\n",
    "\n",
    "index_name = \"sentences\"\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"sentence\": {\"type\": \"text\"},\n",
    "            \"tokens\": {\"type\": \"keyword\"},\n",
    "            \"word2vec_vector\": {\"type\": \"dense_vector\", \"dims\": 300},\n",
    "            \"bert_vector\": {\"type\": \"dense_vector\", \"dims\": 768},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, body=mapping)\n",
    "    print(f\"Created index: {index_name}\")\n",
    "\n",
    "# Load Pre-trained Models\n",
    "import gensim.downloader as api\n",
    "max_attempts = 3\n",
    "for attempt in range(max_attempts):\n",
    "    try:\n",
    "        word2vec_model = api.load('word2vec-google-news-300')\n",
    "        print(\"Word2Vec model loaded successfully!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "        if attempt < max_attempts - 1:\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def get_sentence_vector(tokens, model):\n",
    "    vectors = [model[word] for word in tokens if word in model]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "def get_bert_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "\n",
    "# Index Sentences\n",
    "repository_sentences = [\n",
    "    \"artificial intelligence is transforming industries by automating tasks and improving efficiency\",\n",
    "    \"climate change is a global challenge that requires immediate attention to mitigate its adverse effects\",\n",
    "    \"the rapid advancement of technology has significantly impacted communication and information sharing\",\n",
    "    \"renewable energy sources such as solar and wind are crucial for reducing dependence on fossil fuels\",\n",
    "    \"data privacy and security have become major concerns in the digital age requiring stringent measures\"\n",
    "]\n",
    "\n",
    "def index_sentences(sentences, es, index_name, word2vec_model, bert_model, tokenizer):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        tokens = sentence.lower().split()\n",
    "        w2v_vector = get_sentence_vector(tokens, word2vec_model).tolist()\n",
    "        bert_vector = get_bert_embedding(sentence, tokenizer, bert_model).tolist()\n",
    "        doc = {\n",
    "            \"sentence\": sentence,\n",
    "            \"tokens\": tokens,\n",
    "            \"word2vec_vector\": w2v_vector,\n",
    "            \"bert_vector\": bert_vector\n",
    "        }\n",
    "        es.index(index=index_name, id=i, body=doc)\n",
    "    print(f\"Indexed {len(sentences)} sentences\")\n",
    "\n",
    "es.delete_by_query(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "index_sentences(repository_sentences, es, index_name, word2vec_model, bert_model, tokenizer)\n",
    "\n",
    "# Updated Plagiarism Detection\n",
    "def classify_plagiarism(input_sentence, es, index_name, word2vec_model, bert_model, tokenizer):\n",
    "    input_sentence = re.sub(r'[^\\w\\s]', '', input_sentence.lower())\n",
    "    input_tokens = input_sentence.split()\n",
    "    input_w2v = get_sentence_vector(input_tokens, word2vec_model)\n",
    "    input_bert = get_bert_embedding(input_sentence, tokenizer, bert_model)\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": \"\"\"\n",
    "                        (cosineSimilarity(params.query_w2v, 'word2vec_vector') * 0.4 +\n",
    "                         cosineSimilarity(params.query_bert, 'bert_vector') * 0.6)\n",
    "                    \"\"\",\n",
    "                    \"params\": {\n",
    "                        \"query_w2v\": input_w2v.tolist(),\n",
    "                        \"query_bert\": input_bert.tolist()\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    }\n",
    "    response = es.search(index=index_name, body=query)\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    features = []\n",
    "    valid_hits = []\n",
    "    for hit in hits:\n",
    "        repo_sent = hit[\"_source\"][\"sentence\"]\n",
    "        repo_tok = hit[\"_source\"][\"tokens\"]\n",
    "        w2v_sim = np.dot(input_w2v, hit[\"_source\"][\"word2vec_vector\"]) / (\n",
    "            np.linalg.norm(input_w2v) * np.linalg.norm(hit[\"_source\"][\"word2vec_vector\"]) + 1e-8\n",
    "        )\n",
    "        bert_sim = np.dot(input_bert, hit[\"_source\"][\"bert_vector\"]) / (\n",
    "            np.linalg.norm(input_bert) * np.linalg.norm(hit[\"_source\"][\"bert_vector\"]) + 1e-8\n",
    "        )\n",
    "        word_overlap = len(set(input_tokens) & set(repo_tok)) / len(set(input_tokens))\n",
    "        length_diff = abs(len(input_tokens) - len(repo_tok))\n",
    "        if w2v_sim < 0.6 or word_overlap < 0.2:\n",
    "            continue\n",
    "        features.append([w2v_sim, bert_sim, word_overlap, length_diff])\n",
    "        valid_hits.append(hit)\n",
    "    if not features:\n",
    "        return \"No Match\", None, None, 0.0\n",
    "    features = np.array(features)\n",
    "    combined_sim = features[:, 0] * 0.4 + features[:, 1] * 0.6\n",
    "    best_match_idx = np.argmax(combined_sim)\n",
    "    w2v_sim, bert_sim, word_overlap, length_diff = features[best_match_idx]\n",
    "    if w2v_sim > 0.99 and bert_sim > 0.99 and word_overlap > 0.95 and length_diff == 0:\n",
    "        predicted_class = \"Cut-Paste\"\n",
    "    elif w2v_sim >= 0.85 and bert_sim >= 0.85 and word_overlap >= 0.6:\n",
    "        predicted_class = \"Light Paraphrasing\"\n",
    "    elif w2v_sim >= 0.6 and bert_sim >= 0.6 and word_overlap < 0.6:\n",
    "        predicted_class = \"Heavy Paraphrasing\"\n",
    "    else:\n",
    "        predicted_class = \"No Match\"\n",
    "    similarity_percentage = (w2v_sim * 0.4 + bert_sim * 0.4 + word_overlap * 0.2) * 100\n",
    "    matched_sentence = valid_hits[best_match_idx][\"_source\"][\"sentence\"]\n",
    "    return predicted_class, matched_sentence, features[best_match_idx], similarity_percentage\n",
    "\n",
    "# UI\n",
    "input_text = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter a sentence to check for plagiarism',\n",
    "    description='Input:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "classify_button = widgets.Button(\n",
    "    description='Classify',\n",
    "    button_style='success',\n",
    "    tooltip='Click to classify the sentence',\n",
    ")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_classify_button_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        input_sentence = input_text.value.strip()\n",
    "        if not input_sentence:\n",
    "            print(\"Please enter a sentence.\")\n",
    "            return\n",
    "        predicted_class, matched_sentence, features, similarity_percentage = classify_plagiarism(\n",
    "            input_sentence, es, index_name, word2vec_model, bert_model, tokenizer\n",
    "        )\n",
    "        print(f\"Input Sentence: {input_sentence}\")\n",
    "        print(f\"Matched Sentence: {matched_sentence if matched_sentence else 'None'}\")\n",
    "        print(f\"Predicted Plagiarism Type: {predicted_class}\")\n",
    "        print(f\"Similarity Percentage: {similarity_percentage:.2f}%\")\n",
    "        print(f\"Features (Word2Vec Sim, BERT Sim, Word Overlap, Length Diff): {features if features is not None else 'N/A'}\")\n",
    "\n",
    "classify_button.on_click(on_classify_button_clicked)\n",
    "print(\"Plagiarism Detection Tool\")\n",
    "display(input_text, classify_button, output_area)\n",
    "\n",
    "input_text.value = \"artificial intelligence is modifying industries by automating jobs and enhancing performance\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
